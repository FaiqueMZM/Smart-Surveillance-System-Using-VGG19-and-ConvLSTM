{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62a1e4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Inas\\anaconda3\\envs\\FYP\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Inas\\anaconda3\\envs\\FYP\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/800], Loss: 0.0571\n",
      "Epoch [1/10], Step [20/800], Loss: 0.0013\n",
      "Epoch [1/10], Step [30/800], Loss: 0.0004\n",
      "Epoch [1/10], Step [40/800], Loss: 0.0002\n",
      "Epoch [1/10], Step [50/800], Loss: 0.0002\n",
      "Epoch [1/10], Step [60/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [70/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [80/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [90/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [100/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [110/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [120/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [130/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [140/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [150/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [160/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [170/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [180/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [190/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [200/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [210/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [220/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [230/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [240/800], Loss: 0.0000\n",
      "Epoch [1/10], Step [250/800], Loss: 0.0001\n",
      "Epoch [1/10], Step [260/800], Loss: 0.0000\n",
      "Epoch [1/10], Step [270/800], Loss: 0.0000\n",
      "Epoch [1/10], Step [280/800], Loss: 0.0000\n",
      "Epoch [1/10], Step [290/800], Loss: 0.0000\n",
      "Epoch [1/10], Step [300/800], Loss: 0.0000\n",
      "Epoch [1/10], Step [310/800], Loss: 0.0000\n",
      "Epoch [1/10], Step [320/800], Loss: 0.0000\n",
      "Epoch [1/10], Step [330/800], Loss: 0.0000\n",
      "Epoch [1/10], Step [340/800], Loss: 0.0000\n",
      "Epoch [1/10], Step [350/800], Loss: 0.0000\n",
      "Epoch [1/10], Step [360/800], Loss: 0.0000\n",
      "Epoch [1/10], Step [370/800], Loss: 0.0000\n",
      "Epoch [1/10], Step [380/800], Loss: 0.0000\n",
      "Epoch [1/10], Step [390/800], Loss: 0.0000\n",
      "Epoch [1/10], Step [400/800], Loss: 0.0000\n",
      "Epoch [1/10], Step [410/800], Loss: 1.4293\n",
      "Epoch [1/10], Step [420/800], Loss: 0.2230\n",
      "Epoch [1/10], Step [430/800], Loss: 0.0707\n",
      "Epoch [1/10], Step [440/800], Loss: 0.0279\n",
      "Epoch [1/10], Step [450/800], Loss: 0.0180\n",
      "Epoch [1/10], Step [460/800], Loss: 0.0131\n",
      "Epoch [1/10], Step [470/800], Loss: 0.0105\n",
      "Epoch [1/10], Step [480/800], Loss: 0.0074\n",
      "Epoch [1/10], Step [490/800], Loss: 0.0067\n",
      "Epoch [1/10], Step [500/800], Loss: 0.0066\n",
      "Epoch [1/10], Step [510/800], Loss: 0.0060\n",
      "Epoch [1/10], Step [520/800], Loss: 0.0056\n",
      "Epoch [1/10], Step [530/800], Loss: 0.0053\n",
      "Epoch [1/10], Step [540/800], Loss: 0.0049\n",
      "Epoch [1/10], Step [550/800], Loss: 0.0040\n",
      "Epoch [1/10], Step [560/800], Loss: 0.0040\n",
      "Epoch [1/10], Step [570/800], Loss: 0.0041\n",
      "Epoch [1/10], Step [580/800], Loss: 0.0033\n",
      "Epoch [1/10], Step [590/800], Loss: 0.0032\n",
      "Epoch [1/10], Step [600/800], Loss: 0.0032\n",
      "Epoch [1/10], Step [610/800], Loss: 0.0029\n",
      "Epoch [1/10], Step [620/800], Loss: 0.0024\n",
      "Epoch [1/10], Step [630/800], Loss: 0.0027\n",
      "Epoch [1/10], Step [640/800], Loss: 0.0020\n",
      "Epoch [1/10], Step [650/800], Loss: 0.0022\n",
      "Epoch [1/10], Step [660/800], Loss: 0.0022\n",
      "Epoch [1/10], Step [670/800], Loss: 0.0023\n",
      "Epoch [1/10], Step [680/800], Loss: 0.0021\n",
      "Epoch [1/10], Step [690/800], Loss: 0.0020\n",
      "Epoch [1/10], Step [700/800], Loss: 0.0019\n",
      "Epoch [1/10], Step [710/800], Loss: 0.0019\n",
      "Epoch [1/10], Step [720/800], Loss: 0.0015\n",
      "Epoch [1/10], Step [730/800], Loss: 0.0016\n",
      "Epoch [1/10], Step [740/800], Loss: 0.0016\n",
      "Epoch [1/10], Step [750/800], Loss: 0.0014\n",
      "Epoch [1/10], Step [760/800], Loss: 0.0016\n",
      "Epoch [1/10], Step [770/800], Loss: 0.0015\n",
      "Epoch [1/10], Step [780/800], Loss: 0.0012\n",
      "Epoch [1/10], Step [790/800], Loss: 0.0014\n",
      "Epoch [1/10], Step [800/800], Loss: 0.0015\n",
      "Epoch [2/10], Step [10/800], Loss: 3.7291\n",
      "Epoch [2/10], Step [20/800], Loss: 1.0861\n",
      "Epoch [2/10], Step [30/800], Loss: 0.6187\n",
      "Epoch [2/10], Step [40/800], Loss: 0.3544\n",
      "Epoch [2/10], Step [50/800], Loss: 0.1898\n",
      "Epoch [2/10], Step [60/800], Loss: 0.0929\n",
      "Epoch [2/10], Step [70/800], Loss: 0.0397\n",
      "Epoch [2/10], Step [80/800], Loss: 0.0305\n",
      "Epoch [2/10], Step [90/800], Loss: 0.0252\n",
      "Epoch [2/10], Step [100/800], Loss: 0.0232\n",
      "Epoch [2/10], Step [110/800], Loss: 0.0162\n",
      "Epoch [2/10], Step [120/800], Loss: 0.0144\n",
      "Epoch [2/10], Step [130/800], Loss: 0.0106\n",
      "Epoch [2/10], Step [140/800], Loss: 0.0095\n",
      "Epoch [2/10], Step [150/800], Loss: 0.0094\n",
      "Epoch [2/10], Step [160/800], Loss: 0.0094\n",
      "Epoch [2/10], Step [170/800], Loss: 0.0079\n",
      "Epoch [2/10], Step [180/800], Loss: 0.0063\n",
      "Epoch [2/10], Step [190/800], Loss: 0.0067\n",
      "Epoch [2/10], Step [200/800], Loss: 0.0067\n",
      "Epoch [2/10], Step [210/800], Loss: 0.0059\n",
      "Epoch [2/10], Step [220/800], Loss: 0.0057\n",
      "Epoch [2/10], Step [230/800], Loss: 0.0053\n",
      "Epoch [2/10], Step [240/800], Loss: 0.0043\n",
      "Epoch [2/10], Step [250/800], Loss: 0.0047\n",
      "Epoch [2/10], Step [260/800], Loss: 0.0038\n",
      "Epoch [2/10], Step [270/800], Loss: 0.0037\n",
      "Epoch [2/10], Step [280/800], Loss: 0.0038\n",
      "Epoch [2/10], Step [290/800], Loss: 0.0035\n",
      "Epoch [2/10], Step [300/800], Loss: 0.0041\n",
      "Epoch [2/10], Step [310/800], Loss: 0.0035\n",
      "Epoch [2/10], Step [320/800], Loss: 0.0031\n",
      "Epoch [2/10], Step [330/800], Loss: 0.0028\n",
      "Epoch [2/10], Step [340/800], Loss: 0.0028\n",
      "Epoch [2/10], Step [350/800], Loss: 0.0028\n",
      "Epoch [2/10], Step [360/800], Loss: 0.0025\n",
      "Epoch [2/10], Step [370/800], Loss: 0.0022\n",
      "Epoch [2/10], Step [380/800], Loss: 0.0026\n",
      "Epoch [2/10], Step [390/800], Loss: 0.0023\n",
      "Epoch [2/10], Step [400/800], Loss: 0.0022\n",
      "Epoch [2/10], Step [410/800], Loss: 3.3430\n",
      "Epoch [2/10], Step [420/800], Loss: 1.1196\n",
      "Epoch [2/10], Step [430/800], Loss: 0.5360\n",
      "Epoch [2/10], Step [440/800], Loss: 0.2952\n",
      "Epoch [2/10], Step [450/800], Loss: 0.1554\n",
      "Epoch [2/10], Step [460/800], Loss: 0.0633\n",
      "Epoch [2/10], Step [470/800], Loss: 0.0230\n",
      "Epoch [2/10], Step [480/800], Loss: 0.0134\n",
      "Epoch [2/10], Step [490/800], Loss: 0.0116\n",
      "Epoch [2/10], Step [500/800], Loss: 0.0079\n",
      "Epoch [2/10], Step [510/800], Loss: 0.0068\n",
      "Epoch [2/10], Step [520/800], Loss: 0.0040\n",
      "Epoch [2/10], Step [530/800], Loss: 0.0054\n",
      "Epoch [2/10], Step [540/800], Loss: 0.0033\n",
      "Epoch [2/10], Step [550/800], Loss: 0.0028\n",
      "Epoch [2/10], Step [560/800], Loss: 0.0031\n",
      "Epoch [2/10], Step [570/800], Loss: 0.0030\n",
      "Epoch [2/10], Step [580/800], Loss: 0.0023\n",
      "Epoch [2/10], Step [590/800], Loss: 0.0021\n",
      "Epoch [2/10], Step [600/800], Loss: 0.0021\n",
      "Epoch [2/10], Step [610/800], Loss: 0.0019\n",
      "Epoch [2/10], Step [620/800], Loss: 0.0017\n",
      "Epoch [2/10], Step [630/800], Loss: 0.0016\n",
      "Epoch [2/10], Step [640/800], Loss: 0.0013\n",
      "Epoch [2/10], Step [650/800], Loss: 0.0013\n",
      "Epoch [2/10], Step [660/800], Loss: 0.0013\n",
      "Epoch [2/10], Step [670/800], Loss: 0.0016\n",
      "Epoch [2/10], Step [680/800], Loss: 0.0026\n",
      "Epoch [2/10], Step [690/800], Loss: 0.0018\n",
      "Epoch [2/10], Step [700/800], Loss: 0.0020\n",
      "Epoch [2/10], Step [710/800], Loss: 0.0014\n",
      "Epoch [2/10], Step [720/800], Loss: 0.0010\n",
      "Epoch [2/10], Step [730/800], Loss: 0.0013\n",
      "Epoch [2/10], Step [740/800], Loss: 0.0010\n",
      "Epoch [2/10], Step [750/800], Loss: 0.0008\n",
      "Epoch [2/10], Step [760/800], Loss: 0.0009\n",
      "Epoch [2/10], Step [770/800], Loss: 0.0009\n",
      "Epoch [2/10], Step [780/800], Loss: 0.0008\n",
      "Epoch [2/10], Step [790/800], Loss: 0.0008\n",
      "Epoch [2/10], Step [800/800], Loss: 0.0010\n",
      "Epoch [3/10], Step [10/800], Loss: 2.1221\n",
      "Epoch [3/10], Step [20/800], Loss: 0.5345\n",
      "Epoch [3/10], Step [30/800], Loss: 0.1367\n",
      "Epoch [3/10], Step [40/800], Loss: 0.0301\n",
      "Epoch [3/10], Step [50/800], Loss: 0.0194\n",
      "Epoch [3/10], Step [60/800], Loss: 0.0151\n",
      "Epoch [3/10], Step [70/800], Loss: 0.0085\n",
      "Epoch [3/10], Step [80/800], Loss: 0.0074\n",
      "Epoch [3/10], Step [90/800], Loss: 0.0058\n",
      "Epoch [3/10], Step [100/800], Loss: 0.0086\n",
      "Epoch [3/10], Step [110/800], Loss: 0.0047\n",
      "Epoch [3/10], Step [120/800], Loss: 0.0038\n",
      "Epoch [3/10], Step [130/800], Loss: 0.0045\n",
      "Epoch [3/10], Step [140/800], Loss: 0.0035\n",
      "Epoch [3/10], Step [150/800], Loss: 0.0032\n",
      "Epoch [3/10], Step [160/800], Loss: 0.0042\n",
      "Epoch [3/10], Step [170/800], Loss: 0.0035\n",
      "Epoch [3/10], Step [180/800], Loss: 0.0026\n",
      "Epoch [3/10], Step [190/800], Loss: 0.0027\n",
      "Epoch [3/10], Step [200/800], Loss: 0.0030\n",
      "Epoch [3/10], Step [210/800], Loss: 0.0026\n",
      "Epoch [3/10], Step [220/800], Loss: 0.0022\n",
      "Epoch [3/10], Step [230/800], Loss: 0.0025\n",
      "Epoch [3/10], Step [240/800], Loss: 0.0019\n",
      "Epoch [3/10], Step [250/800], Loss: 0.0021\n",
      "Epoch [3/10], Step [260/800], Loss: 0.0016\n",
      "Epoch [3/10], Step [270/800], Loss: 0.0020\n",
      "Epoch [3/10], Step [280/800], Loss: 0.0022\n",
      "Epoch [3/10], Step [290/800], Loss: 0.0014\n",
      "Epoch [3/10], Step [300/800], Loss: 0.0019\n",
      "Epoch [3/10], Step [310/800], Loss: 0.0020\n",
      "Epoch [3/10], Step [320/800], Loss: 0.0013\n",
      "Epoch [3/10], Step [330/800], Loss: 0.0016\n",
      "Epoch [3/10], Step [340/800], Loss: 0.0014\n",
      "Epoch [3/10], Step [350/800], Loss: 0.0016\n",
      "Epoch [3/10], Step [360/800], Loss: 0.0016\n",
      "Epoch [3/10], Step [370/800], Loss: 0.0011\n",
      "Epoch [3/10], Step [380/800], Loss: 0.0018\n",
      "Epoch [3/10], Step [390/800], Loss: 0.0012\n",
      "Epoch [3/10], Step [400/800], Loss: 0.0010\n",
      "Epoch [3/10], Step [410/800], Loss: 2.3804\n",
      "Epoch [3/10], Step [420/800], Loss: 0.5958\n",
      "Epoch [3/10], Step [430/800], Loss: 0.3858\n",
      "Epoch [3/10], Step [440/800], Loss: 0.2149\n",
      "Epoch [3/10], Step [450/800], Loss: 0.1445\n",
      "Epoch [3/10], Step [460/800], Loss: 0.1002\n",
      "Epoch [3/10], Step [470/800], Loss: 0.0596\n",
      "Epoch [3/10], Step [480/800], Loss: 0.0442\n",
      "Epoch [3/10], Step [490/800], Loss: 0.0385\n",
      "Epoch [3/10], Step [500/800], Loss: 0.0349\n",
      "Epoch [3/10], Step [510/800], Loss: 0.0285\n",
      "Epoch [3/10], Step [520/800], Loss: 0.0223\n",
      "Epoch [3/10], Step [530/800], Loss: 0.0243\n",
      "Epoch [3/10], Step [540/800], Loss: 0.0172\n",
      "Epoch [3/10], Step [550/800], Loss: 0.0149\n",
      "Epoch [3/10], Step [560/800], Loss: 0.0144\n",
      "Epoch [3/10], Step [570/800], Loss: 0.0143\n",
      "Epoch [3/10], Step [580/800], Loss: 0.0106\n",
      "Epoch [3/10], Step [590/800], Loss: 0.0106\n",
      "Epoch [3/10], Step [600/800], Loss: 0.0104\n",
      "Epoch [3/10], Step [610/800], Loss: 0.0095\n",
      "Epoch [3/10], Step [620/800], Loss: 0.0070\n",
      "Epoch [3/10], Step [630/800], Loss: 0.0082\n",
      "Epoch [3/10], Step [640/800], Loss: 0.0062\n",
      "Epoch [3/10], Step [650/800], Loss: 0.0062\n",
      "Epoch [3/10], Step [660/800], Loss: 0.0063\n",
      "Epoch [3/10], Step [670/800], Loss: 0.0072\n",
      "Epoch [3/10], Step [680/800], Loss: 0.0076\n",
      "Epoch [3/10], Step [690/800], Loss: 0.0063\n",
      "Epoch [3/10], Step [700/800], Loss: 0.0062\n",
      "Epoch [3/10], Step [710/800], Loss: 0.0062\n",
      "Epoch [3/10], Step [720/800], Loss: 0.0040\n",
      "Epoch [3/10], Step [730/800], Loss: 0.0051\n",
      "Epoch [3/10], Step [740/800], Loss: 0.0046\n",
      "Epoch [3/10], Step [750/800], Loss: 0.0040\n",
      "Epoch [3/10], Step [760/800], Loss: 0.0043\n",
      "Epoch [3/10], Step [770/800], Loss: 0.0043\n",
      "Epoch [3/10], Step [780/800], Loss: 0.0037\n",
      "Epoch [3/10], Step [790/800], Loss: 0.0043\n",
      "Epoch [3/10], Step [800/800], Loss: 0.0041\n",
      "Epoch [4/10], Step [10/800], Loss: 3.0883\n",
      "Epoch [4/10], Step [20/800], Loss: 1.1404\n",
      "Epoch [4/10], Step [30/800], Loss: 0.6361\n",
      "Epoch [4/10], Step [40/800], Loss: 0.4229\n",
      "Epoch [4/10], Step [50/800], Loss: 0.2428\n",
      "Epoch [4/10], Step [60/800], Loss: 0.1526\n",
      "Epoch [4/10], Step [70/800], Loss: 0.0834\n",
      "Epoch [4/10], Step [80/800], Loss: 0.0633\n",
      "Epoch [4/10], Step [90/800], Loss: 0.0446\n",
      "Epoch [4/10], Step [100/800], Loss: 0.0491\n",
      "Epoch [4/10], Step [110/800], Loss: 0.0259\n",
      "Epoch [4/10], Step [120/800], Loss: 0.0236\n",
      "Epoch [4/10], Step [130/800], Loss: 0.0203\n",
      "Epoch [4/10], Step [140/800], Loss: 0.0165\n",
      "Epoch [4/10], Step [150/800], Loss: 0.0152\n",
      "Epoch [4/10], Step [160/800], Loss: 0.0157\n",
      "Epoch [4/10], Step [170/800], Loss: 0.0116\n",
      "Epoch [4/10], Step [180/800], Loss: 0.0111\n",
      "Epoch [4/10], Step [190/800], Loss: 0.0102\n",
      "Epoch [4/10], Step [200/800], Loss: 0.0098\n",
      "Epoch [4/10], Step [210/800], Loss: 0.0088\n",
      "Epoch [4/10], Step [220/800], Loss: 0.0081\n",
      "Epoch [4/10], Step [230/800], Loss: 0.0080\n",
      "Epoch [4/10], Step [240/800], Loss: 0.0062\n",
      "Epoch [4/10], Step [250/800], Loss: 0.0068\n",
      "Epoch [4/10], Step [260/800], Loss: 0.0061\n",
      "Epoch [4/10], Step [270/800], Loss: 0.0061\n",
      "Epoch [4/10], Step [280/800], Loss: 0.0061\n",
      "Epoch [4/10], Step [290/800], Loss: 0.0052\n",
      "Epoch [4/10], Step [300/800], Loss: 0.0060\n",
      "Epoch [4/10], Step [310/800], Loss: 0.0053\n",
      "Epoch [4/10], Step [320/800], Loss: 0.0053\n",
      "Epoch [4/10], Step [330/800], Loss: 0.0044\n",
      "Epoch [4/10], Step [340/800], Loss: 0.0043\n",
      "Epoch [4/10], Step [350/800], Loss: 0.0044\n",
      "Epoch [4/10], Step [360/800], Loss: 0.0042\n",
      "Epoch [4/10], Step [370/800], Loss: 0.0038\n",
      "Epoch [4/10], Step [380/800], Loss: 0.0045\n",
      "Epoch [4/10], Step [390/800], Loss: 0.0038\n",
      "Epoch [4/10], Step [400/800], Loss: 0.0035\n",
      "Epoch [4/10], Step [410/800], Loss: 2.3361\n",
      "Epoch [4/10], Step [420/800], Loss: 0.6704\n",
      "Epoch [4/10], Step [430/800], Loss: 0.5002\n",
      "Epoch [4/10], Step [440/800], Loss: 0.2840\n",
      "Epoch [4/10], Step [450/800], Loss: 0.1681\n",
      "Epoch [4/10], Step [460/800], Loss: 0.1352\n",
      "Epoch [4/10], Step [470/800], Loss: 0.0894\n",
      "Epoch [4/10], Step [480/800], Loss: 0.0635\n",
      "Epoch [4/10], Step [490/800], Loss: 0.0505\n",
      "Epoch [4/10], Step [500/800], Loss: 0.0434\n",
      "Epoch [4/10], Step [510/800], Loss: 0.0304\n",
      "Epoch [4/10], Step [520/800], Loss: 0.0248\n",
      "Epoch [4/10], Step [530/800], Loss: 0.0242\n",
      "Epoch [4/10], Step [540/800], Loss: 0.0181\n",
      "Epoch [4/10], Step [550/800], Loss: 0.0155\n",
      "Epoch [4/10], Step [560/800], Loss: 0.0144\n",
      "Epoch [4/10], Step [570/800], Loss: 0.0144\n",
      "Epoch [4/10], Step [580/800], Loss: 0.0110\n",
      "Epoch [4/10], Step [590/800], Loss: 0.0113\n",
      "Epoch [4/10], Step [600/800], Loss: 0.0101\n",
      "Epoch [4/10], Step [610/800], Loss: 0.0092\n",
      "Epoch [4/10], Step [620/800], Loss: 0.0069\n",
      "Epoch [4/10], Step [630/800], Loss: 0.0081\n",
      "Epoch [4/10], Step [640/800], Loss: 0.0067\n",
      "Epoch [4/10], Step [650/800], Loss: 0.0062\n",
      "Epoch [4/10], Step [660/800], Loss: 0.0060\n",
      "Epoch [4/10], Step [670/800], Loss: 0.0066\n",
      "Epoch [4/10], Step [680/800], Loss: 0.0072\n",
      "Epoch [4/10], Step [690/800], Loss: 0.0058\n",
      "Epoch [4/10], Step [700/800], Loss: 0.0057\n",
      "Epoch [4/10], Step [710/800], Loss: 0.0058\n",
      "Epoch [4/10], Step [720/800], Loss: 0.0044\n",
      "Epoch [4/10], Step [730/800], Loss: 0.0048\n",
      "Epoch [4/10], Step [740/800], Loss: 0.0043\n",
      "Epoch [4/10], Step [750/800], Loss: 0.0037\n",
      "Epoch [4/10], Step [760/800], Loss: 0.0039\n",
      "Epoch [4/10], Step [770/800], Loss: 0.0042\n",
      "Epoch [4/10], Step [780/800], Loss: 0.0041\n",
      "Epoch [4/10], Step [790/800], Loss: 0.0041\n",
      "Epoch [4/10], Step [800/800], Loss: 0.0037\n",
      "Epoch [5/10], Step [10/800], Loss: 2.0983\n",
      "Epoch [5/10], Step [20/800], Loss: 0.7101\n",
      "Epoch [5/10], Step [30/800], Loss: 0.4794\n",
      "Epoch [5/10], Step [40/800], Loss: 0.5159\n",
      "Epoch [5/10], Step [50/800], Loss: 0.1986\n",
      "Epoch [5/10], Step [60/800], Loss: 0.1361\n",
      "Epoch [5/10], Step [70/800], Loss: 0.0583\n",
      "Epoch [5/10], Step [80/800], Loss: 0.0556\n",
      "Epoch [5/10], Step [90/800], Loss: 0.0421\n",
      "Epoch [5/10], Step [100/800], Loss: 0.0890\n",
      "Epoch [5/10], Step [110/800], Loss: 0.0252\n",
      "Epoch [5/10], Step [120/800], Loss: 0.0252\n",
      "Epoch [5/10], Step [130/800], Loss: 0.0230\n",
      "Epoch [5/10], Step [140/800], Loss: 0.0177\n",
      "Epoch [5/10], Step [150/800], Loss: 0.0201\n",
      "Epoch [5/10], Step [160/800], Loss: 0.0176\n",
      "Epoch [5/10], Step [170/800], Loss: 0.0132\n",
      "Epoch [5/10], Step [180/800], Loss: 0.0138\n",
      "Epoch [5/10], Step [190/800], Loss: 0.0121\n",
      "Epoch [5/10], Step [200/800], Loss: 0.0117\n",
      "Epoch [5/10], Step [210/800], Loss: 0.0107\n",
      "Epoch [5/10], Step [220/800], Loss: 0.0102\n",
      "Epoch [5/10], Step [230/800], Loss: 0.0101\n",
      "Epoch [5/10], Step [240/800], Loss: 0.0078\n",
      "Epoch [5/10], Step [250/800], Loss: 0.0087\n",
      "Epoch [5/10], Step [260/800], Loss: 0.0076\n",
      "Epoch [5/10], Step [270/800], Loss: 0.0073\n",
      "Epoch [5/10], Step [280/800], Loss: 0.0073\n",
      "Epoch [5/10], Step [290/800], Loss: 0.0065\n",
      "Epoch [5/10], Step [300/800], Loss: 0.0075\n",
      "Epoch [5/10], Step [310/800], Loss: 0.0067\n",
      "Epoch [5/10], Step [320/800], Loss: 0.0069\n",
      "Epoch [5/10], Step [330/800], Loss: 0.0051\n",
      "Epoch [5/10], Step [340/800], Loss: 0.0050\n",
      "Epoch [5/10], Step [350/800], Loss: 0.0053\n",
      "Epoch [5/10], Step [360/800], Loss: 0.0050\n",
      "Epoch [5/10], Step [370/800], Loss: 0.0046\n",
      "Epoch [5/10], Step [380/800], Loss: 0.0060\n",
      "Epoch [5/10], Step [390/800], Loss: 0.0049\n",
      "Epoch [5/10], Step [400/800], Loss: 0.0040\n",
      "Epoch [5/10], Step [410/800], Loss: 1.8214\n",
      "Epoch [5/10], Step [420/800], Loss: 0.6305\n",
      "Epoch [5/10], Step [430/800], Loss: 0.6325\n",
      "Epoch [5/10], Step [440/800], Loss: 0.1468\n",
      "Epoch [5/10], Step [450/800], Loss: 0.0639\n",
      "Epoch [5/10], Step [460/800], Loss: 0.0641\n",
      "Epoch [5/10], Step [470/800], Loss: 0.0439\n",
      "Epoch [5/10], Step [480/800], Loss: 0.0318\n",
      "Epoch [5/10], Step [490/800], Loss: 0.0246\n",
      "Epoch [5/10], Step [500/800], Loss: 0.0294\n",
      "Epoch [5/10], Step [510/800], Loss: 0.0159\n",
      "Epoch [5/10], Step [520/800], Loss: 0.0136\n",
      "Epoch [5/10], Step [530/800], Loss: 0.0153\n",
      "Epoch [5/10], Step [540/800], Loss: 0.0105\n",
      "Epoch [5/10], Step [550/800], Loss: 0.0091\n",
      "Epoch [5/10], Step [560/800], Loss: 0.0078\n",
      "Epoch [5/10], Step [570/800], Loss: 0.0099\n",
      "Epoch [5/10], Step [580/800], Loss: 0.0062\n",
      "Epoch [5/10], Step [590/800], Loss: 0.0080\n",
      "Epoch [5/10], Step [600/800], Loss: 0.0067\n",
      "Epoch [5/10], Step [610/800], Loss: 0.0064\n",
      "Epoch [5/10], Step [620/800], Loss: 0.0036\n",
      "Epoch [5/10], Step [630/800], Loss: 0.0053\n",
      "Epoch [5/10], Step [640/800], Loss: 0.0038\n",
      "Epoch [5/10], Step [650/800], Loss: 0.0033\n",
      "Epoch [5/10], Step [660/800], Loss: 0.0035\n",
      "Epoch [5/10], Step [670/800], Loss: 0.0040\n",
      "Epoch [5/10], Step [680/800], Loss: 0.0044\n",
      "Epoch [5/10], Step [690/800], Loss: 0.0037\n",
      "Epoch [5/10], Step [700/800], Loss: 0.0038\n",
      "Epoch [5/10], Step [710/800], Loss: 0.0048\n",
      "Epoch [5/10], Step [720/800], Loss: 0.0028\n",
      "Epoch [5/10], Step [730/800], Loss: 0.0027\n",
      "Epoch [5/10], Step [740/800], Loss: 0.0028\n",
      "Epoch [5/10], Step [750/800], Loss: 0.0020\n",
      "Epoch [5/10], Step [760/800], Loss: 0.0024\n",
      "Epoch [5/10], Step [770/800], Loss: 0.0030\n",
      "Epoch [5/10], Step [780/800], Loss: 0.0030\n",
      "Epoch [5/10], Step [790/800], Loss: 0.0033\n",
      "Epoch [5/10], Step [800/800], Loss: 0.0026\n",
      "Epoch [6/10], Step [10/800], Loss: 1.1314\n",
      "Epoch [6/10], Step [20/800], Loss: 0.2720\n",
      "Epoch [6/10], Step [30/800], Loss: 0.1541\n",
      "Epoch [6/10], Step [40/800], Loss: 0.5731\n",
      "Epoch [6/10], Step [50/800], Loss: 0.0822\n",
      "Epoch [6/10], Step [60/800], Loss: 0.1063\n",
      "Epoch [6/10], Step [70/800], Loss: 0.0288\n",
      "Epoch [6/10], Step [80/800], Loss: 0.0487\n",
      "Epoch [6/10], Step [90/800], Loss: 0.0278\n",
      "Epoch [6/10], Step [100/800], Loss: 0.3051\n",
      "Epoch [6/10], Step [110/800], Loss: 0.0167\n",
      "Epoch [6/10], Step [120/800], Loss: 0.0209\n",
      "Epoch [6/10], Step [130/800], Loss: 0.0166\n",
      "Epoch [6/10], Step [140/800], Loss: 0.0131\n",
      "Epoch [6/10], Step [150/800], Loss: 0.0206\n",
      "Epoch [6/10], Step [160/800], Loss: 0.0141\n",
      "Epoch [6/10], Step [170/800], Loss: 0.0097\n",
      "Epoch [6/10], Step [180/800], Loss: 0.0110\n",
      "Epoch [6/10], Step [190/800], Loss: 0.0096\n",
      "Epoch [6/10], Step [200/800], Loss: 0.0095\n",
      "Epoch [6/10], Step [210/800], Loss: 0.0086\n",
      "Epoch [6/10], Step [220/800], Loss: 0.0082\n",
      "Epoch [6/10], Step [230/800], Loss: 0.0081\n",
      "Epoch [6/10], Step [240/800], Loss: 0.0065\n",
      "Epoch [6/10], Step [250/800], Loss: 0.0073\n",
      "Epoch [6/10], Step [260/800], Loss: 0.0061\n",
      "Epoch [6/10], Step [270/800], Loss: 0.0058\n",
      "Epoch [6/10], Step [280/800], Loss: 0.0062\n",
      "Epoch [6/10], Step [290/800], Loss: 0.0055\n",
      "Epoch [6/10], Step [300/800], Loss: 0.0068\n",
      "Epoch [6/10], Step [310/800], Loss: 0.0060\n",
      "Epoch [6/10], Step [320/800], Loss: 0.0069\n",
      "Epoch [6/10], Step [330/800], Loss: 0.0045\n",
      "Epoch [6/10], Step [340/800], Loss: 0.0045\n",
      "Epoch [6/10], Step [350/800], Loss: 0.0047\n",
      "Epoch [6/10], Step [360/800], Loss: 0.0044\n",
      "Epoch [6/10], Step [370/800], Loss: 0.0039\n",
      "Epoch [6/10], Step [380/800], Loss: 0.0059\n",
      "Epoch [6/10], Step [390/800], Loss: 0.0043\n",
      "Epoch [6/10], Step [400/800], Loss: 0.0037\n",
      "Epoch [6/10], Step [410/800], Loss: 1.1659\n",
      "Epoch [6/10], Step [420/800], Loss: 0.5398\n",
      "Epoch [6/10], Step [430/800], Loss: 1.1897\n",
      "Epoch [6/10], Step [440/800], Loss: 0.1488\n",
      "Epoch [6/10], Step [450/800], Loss: 0.0312\n",
      "Epoch [6/10], Step [460/800], Loss: 0.0458\n",
      "Epoch [6/10], Step [470/800], Loss: 0.0329\n",
      "Epoch [6/10], Step [480/800], Loss: 0.0223\n",
      "Epoch [6/10], Step [490/800], Loss: 0.0163\n",
      "Epoch [6/10], Step [500/800], Loss: 0.0282\n",
      "Epoch [6/10], Step [510/800], Loss: 0.0084\n",
      "Epoch [6/10], Step [520/800], Loss: 0.0081\n",
      "Epoch [6/10], Step [530/800], Loss: 0.0085\n",
      "Epoch [6/10], Step [540/800], Loss: 0.0060\n",
      "Epoch [6/10], Step [550/800], Loss: 0.0056\n",
      "Epoch [6/10], Step [560/800], Loss: 0.0038\n",
      "Epoch [6/10], Step [570/800], Loss: 0.0068\n",
      "Epoch [6/10], Step [580/800], Loss: 0.0037\n",
      "Epoch [6/10], Step [590/800], Loss: 0.0060\n",
      "Epoch [6/10], Step [600/800], Loss: 0.0044\n",
      "Epoch [6/10], Step [610/800], Loss: 0.0044\n",
      "Epoch [6/10], Step [620/800], Loss: 0.0021\n",
      "Epoch [6/10], Step [630/800], Loss: 0.0036\n",
      "Epoch [6/10], Step [640/800], Loss: 0.0026\n",
      "Epoch [6/10], Step [650/800], Loss: 0.0019\n",
      "Epoch [6/10], Step [660/800], Loss: 0.0020\n",
      "Epoch [6/10], Step [670/800], Loss: 0.0022\n",
      "Epoch [6/10], Step [680/800], Loss: 0.0023\n",
      "Epoch [6/10], Step [690/800], Loss: 0.0023\n",
      "Epoch [6/10], Step [700/800], Loss: 0.0019\n",
      "Epoch [6/10], Step [710/800], Loss: 0.0034\n",
      "Epoch [6/10], Step [720/800], Loss: 0.0022\n",
      "Epoch [6/10], Step [730/800], Loss: 0.0012\n",
      "Epoch [6/10], Step [740/800], Loss: 0.0016\n",
      "Epoch [6/10], Step [750/800], Loss: 0.0010\n",
      "Epoch [6/10], Step [760/800], Loss: 0.0014\n",
      "Epoch [6/10], Step [770/800], Loss: 0.0021\n",
      "Epoch [6/10], Step [780/800], Loss: 0.0026\n",
      "Epoch [6/10], Step [790/800], Loss: 0.0030\n",
      "Epoch [6/10], Step [800/800], Loss: 0.0015\n",
      "Epoch [7/10], Step [10/800], Loss: 1.0552\n",
      "Epoch [7/10], Step [20/800], Loss: 0.1404\n",
      "Epoch [7/10], Step [30/800], Loss: 0.0726\n",
      "Epoch [7/10], Step [40/800], Loss: 0.6651\n",
      "Epoch [7/10], Step [50/800], Loss: 0.0466\n",
      "Epoch [7/10], Step [60/800], Loss: 0.2093\n",
      "Epoch [7/10], Step [70/800], Loss: 0.0139\n",
      "Epoch [7/10], Step [80/800], Loss: 0.0428\n",
      "Epoch [7/10], Step [90/800], Loss: 0.0190\n",
      "Epoch [7/10], Step [100/800], Loss: 0.4740\n",
      "Epoch [7/10], Step [110/800], Loss: 0.0093\n",
      "Epoch [7/10], Step [120/800], Loss: 0.0182\n",
      "Epoch [7/10], Step [130/800], Loss: 0.0106\n",
      "Epoch [7/10], Step [140/800], Loss: 0.0072\n",
      "Epoch [7/10], Step [150/800], Loss: 0.0358\n",
      "Epoch [7/10], Step [160/800], Loss: 0.0089\n",
      "Epoch [7/10], Step [170/800], Loss: 0.0052\n",
      "Epoch [7/10], Step [180/800], Loss: 0.0072\n",
      "Epoch [7/10], Step [190/800], Loss: 0.0057\n",
      "Epoch [7/10], Step [200/800], Loss: 0.0056\n",
      "Epoch [7/10], Step [210/800], Loss: 0.0047\n",
      "Epoch [7/10], Step [220/800], Loss: 0.0049\n",
      "Epoch [7/10], Step [230/800], Loss: 0.0050\n",
      "Epoch [7/10], Step [240/800], Loss: 0.0037\n",
      "Epoch [7/10], Step [250/800], Loss: 0.0045\n",
      "Epoch [7/10], Step [260/800], Loss: 0.0034\n",
      "Epoch [7/10], Step [270/800], Loss: 0.0030\n",
      "Epoch [7/10], Step [280/800], Loss: 0.0036\n",
      "Epoch [7/10], Step [290/800], Loss: 0.0032\n",
      "Epoch [7/10], Step [300/800], Loss: 0.0048\n",
      "Epoch [7/10], Step [310/800], Loss: 0.0038\n",
      "Epoch [7/10], Step [320/800], Loss: 0.0090\n",
      "Epoch [7/10], Step [330/800], Loss: 0.0023\n",
      "Epoch [7/10], Step [340/800], Loss: 0.0021\n",
      "Epoch [7/10], Step [350/800], Loss: 0.0026\n",
      "Epoch [7/10], Step [360/800], Loss: 0.0026\n",
      "Epoch [7/10], Step [370/800], Loss: 0.0021\n",
      "Epoch [7/10], Step [380/800], Loss: 0.0030\n",
      "Epoch [7/10], Step [390/800], Loss: 0.0031\n",
      "Epoch [7/10], Step [400/800], Loss: 0.0018\n",
      "Epoch [7/10], Step [410/800], Loss: 1.0807\n",
      "Epoch [7/10], Step [420/800], Loss: 0.6134\n",
      "Epoch [7/10], Step [430/800], Loss: 1.5499\n",
      "Epoch [7/10], Step [440/800], Loss: 0.1519\n",
      "Epoch [7/10], Step [450/800], Loss: 0.0154\n",
      "Epoch [7/10], Step [460/800], Loss: 0.0285\n",
      "Epoch [7/10], Step [470/800], Loss: 0.0205\n",
      "Epoch [7/10], Step [480/800], Loss: 0.0137\n",
      "Epoch [7/10], Step [490/800], Loss: 0.0092\n",
      "Epoch [7/10], Step [500/800], Loss: 0.0238\n",
      "Epoch [7/10], Step [510/800], Loss: 0.0043\n",
      "Epoch [7/10], Step [520/800], Loss: 0.0044\n",
      "Epoch [7/10], Step [530/800], Loss: 0.0048\n",
      "Epoch [7/10], Step [540/800], Loss: 0.0033\n",
      "Epoch [7/10], Step [550/800], Loss: 0.0034\n",
      "Epoch [7/10], Step [560/800], Loss: 0.0020\n",
      "Epoch [7/10], Step [570/800], Loss: 0.0040\n",
      "Epoch [7/10], Step [580/800], Loss: 0.0023\n",
      "Epoch [7/10], Step [590/800], Loss: 0.0041\n",
      "Epoch [7/10], Step [600/800], Loss: 0.0028\n",
      "Epoch [7/10], Step [610/800], Loss: 0.0030\n",
      "Epoch [7/10], Step [620/800], Loss: 0.0014\n",
      "Epoch [7/10], Step [630/800], Loss: 0.0029\n",
      "Epoch [7/10], Step [640/800], Loss: 0.0033\n",
      "Epoch [7/10], Step [650/800], Loss: 0.0017\n",
      "Epoch [7/10], Step [660/800], Loss: 0.0015\n",
      "Epoch [7/10], Step [670/800], Loss: 0.0017\n",
      "Epoch [7/10], Step [680/800], Loss: 0.0014\n",
      "Epoch [7/10], Step [690/800], Loss: 0.0016\n",
      "Epoch [7/10], Step [700/800], Loss: 0.0012\n",
      "Epoch [7/10], Step [710/800], Loss: 0.0021\n",
      "Epoch [7/10], Step [720/800], Loss: 0.0025\n",
      "Epoch [7/10], Step [730/800], Loss: 0.0009\n",
      "Epoch [7/10], Step [740/800], Loss: 0.0013\n",
      "Epoch [7/10], Step [750/800], Loss: 0.0008\n",
      "Epoch [7/10], Step [760/800], Loss: 0.0012\n",
      "Epoch [7/10], Step [770/800], Loss: 0.0024\n",
      "Epoch [7/10], Step [780/800], Loss: 0.0047\n",
      "Epoch [7/10], Step [790/800], Loss: 0.0035\n",
      "Epoch [7/10], Step [800/800], Loss: 0.0012\n",
      "Epoch [8/10], Step [10/800], Loss: 0.8457\n",
      "Epoch [8/10], Step [20/800], Loss: 0.0919\n",
      "Epoch [8/10], Step [30/800], Loss: 0.0421\n",
      "Epoch [8/10], Step [40/800], Loss: 0.9078\n",
      "Epoch [8/10], Step [50/800], Loss: 0.0311\n",
      "Epoch [8/10], Step [60/800], Loss: 0.1995\n",
      "Epoch [8/10], Step [70/800], Loss: 0.0069\n",
      "Epoch [8/10], Step [80/800], Loss: 0.0603\n",
      "Epoch [8/10], Step [90/800], Loss: 0.0108\n",
      "Epoch [8/10], Step [100/800], Loss: 0.5446\n",
      "Epoch [8/10], Step [110/800], Loss: 0.0044\n",
      "Epoch [8/10], Step [120/800], Loss: 0.0152\n",
      "Epoch [8/10], Step [130/800], Loss: 0.0059\n",
      "Epoch [8/10], Step [140/800], Loss: 0.0039\n",
      "Epoch [8/10], Step [150/800], Loss: 0.0610\n",
      "Epoch [8/10], Step [160/800], Loss: 0.0055\n",
      "Epoch [8/10], Step [170/800], Loss: 0.0024\n",
      "Epoch [8/10], Step [180/800], Loss: 0.0053\n",
      "Epoch [8/10], Step [190/800], Loss: 0.0030\n",
      "Epoch [8/10], Step [200/800], Loss: 0.0030\n",
      "Epoch [8/10], Step [210/800], Loss: 0.0025\n",
      "Epoch [8/10], Step [220/800], Loss: 0.0024\n",
      "Epoch [8/10], Step [230/800], Loss: 0.0025\n",
      "Epoch [8/10], Step [240/800], Loss: 0.0028\n",
      "Epoch [8/10], Step [250/800], Loss: 0.0026\n",
      "Epoch [8/10], Step [260/800], Loss: 0.0016\n",
      "Epoch [8/10], Step [270/800], Loss: 0.0015\n",
      "Epoch [8/10], Step [280/800], Loss: 0.0019\n",
      "Epoch [8/10], Step [290/800], Loss: 0.0017\n",
      "Epoch [8/10], Step [300/800], Loss: 0.0032\n",
      "Epoch [8/10], Step [310/800], Loss: 0.0024\n",
      "Epoch [8/10], Step [320/800], Loss: 0.0073\n",
      "Epoch [8/10], Step [330/800], Loss: 0.0013\n",
      "Epoch [8/10], Step [340/800], Loss: 0.0013\n",
      "Epoch [8/10], Step [350/800], Loss: 0.0014\n",
      "Epoch [8/10], Step [360/800], Loss: 0.0015\n",
      "Epoch [8/10], Step [370/800], Loss: 0.0012\n",
      "Epoch [8/10], Step [380/800], Loss: 0.0038\n",
      "Epoch [8/10], Step [390/800], Loss: 0.0019\n",
      "Epoch [8/10], Step [400/800], Loss: 0.0011\n",
      "Epoch [8/10], Step [410/800], Loss: 0.7696\n",
      "Epoch [8/10], Step [420/800], Loss: 0.3919\n",
      "Epoch [8/10], Step [430/800], Loss: 1.7764\n",
      "Epoch [8/10], Step [440/800], Loss: 0.1576\n",
      "Epoch [8/10], Step [450/800], Loss: 0.0096\n",
      "Epoch [8/10], Step [460/800], Loss: 0.0276\n",
      "Epoch [8/10], Step [470/800], Loss: 0.0188\n",
      "Epoch [8/10], Step [480/800], Loss: 0.0106\n",
      "Epoch [8/10], Step [490/800], Loss: 0.0087\n",
      "Epoch [8/10], Step [500/800], Loss: 0.0265\n",
      "Epoch [8/10], Step [510/800], Loss: 0.0027\n",
      "Epoch [8/10], Step [520/800], Loss: 0.0025\n",
      "Epoch [8/10], Step [530/800], Loss: 0.0029\n",
      "Epoch [8/10], Step [540/800], Loss: 0.0016\n",
      "Epoch [8/10], Step [550/800], Loss: 0.0019\n",
      "Epoch [8/10], Step [560/800], Loss: 0.0008\n",
      "Epoch [8/10], Step [570/800], Loss: 0.0025\n",
      "Epoch [8/10], Step [580/800], Loss: 0.0016\n",
      "Epoch [8/10], Step [590/800], Loss: 0.0019\n",
      "Epoch [8/10], Step [600/800], Loss: 0.0015\n",
      "Epoch [8/10], Step [610/800], Loss: 0.0017\n",
      "Epoch [8/10], Step [620/800], Loss: 0.0010\n",
      "Epoch [8/10], Step [630/800], Loss: 0.0013\n",
      "Epoch [8/10], Step [640/800], Loss: 0.0017\n",
      "Epoch [8/10], Step [650/800], Loss: 0.0006\n",
      "Epoch [8/10], Step [660/800], Loss: 0.0006\n",
      "Epoch [8/10], Step [670/800], Loss: 0.0006\n",
      "Epoch [8/10], Step [680/800], Loss: 0.0006\n",
      "Epoch [8/10], Step [690/800], Loss: 0.0009\n",
      "Epoch [8/10], Step [700/800], Loss: 0.0005\n",
      "Epoch [8/10], Step [710/800], Loss: 0.0012\n",
      "Epoch [8/10], Step [720/800], Loss: 0.0014\n",
      "Epoch [8/10], Step [730/800], Loss: 0.0003\n",
      "Epoch [8/10], Step [740/800], Loss: 0.0005\n",
      "Epoch [8/10], Step [750/800], Loss: 0.0002\n",
      "Epoch [8/10], Step [760/800], Loss: 0.0005\n",
      "Epoch [8/10], Step [770/800], Loss: 0.0014\n",
      "Epoch [8/10], Step [780/800], Loss: 0.0025\n",
      "Epoch [8/10], Step [790/800], Loss: 0.0014\n",
      "Epoch [8/10], Step [800/800], Loss: 0.0004\n",
      "Epoch [9/10], Step [10/800], Loss: 0.5793\n",
      "Epoch [9/10], Step [20/800], Loss: 0.0887\n",
      "Epoch [9/10], Step [30/800], Loss: 0.0551\n",
      "Epoch [9/10], Step [40/800], Loss: 0.5776\n",
      "Epoch [9/10], Step [50/800], Loss: 0.0358\n",
      "Epoch [9/10], Step [60/800], Loss: 0.1426\n",
      "Epoch [9/10], Step [70/800], Loss: 0.0088\n",
      "Epoch [9/10], Step [80/800], Loss: 0.0597\n",
      "Epoch [9/10], Step [90/800], Loss: 0.0145\n",
      "Epoch [9/10], Step [100/800], Loss: 0.3236\n",
      "Epoch [9/10], Step [110/800], Loss: 0.0065\n",
      "Epoch [9/10], Step [120/800], Loss: 0.0285\n",
      "Epoch [9/10], Step [130/800], Loss: 0.0087\n",
      "Epoch [9/10], Step [140/800], Loss: 0.0056\n",
      "Epoch [9/10], Step [150/800], Loss: 0.0788\n",
      "Epoch [9/10], Step [160/800], Loss: 0.0071\n",
      "Epoch [9/10], Step [170/800], Loss: 0.0030\n",
      "Epoch [9/10], Step [180/800], Loss: 0.0084\n",
      "Epoch [9/10], Step [190/800], Loss: 0.0047\n",
      "Epoch [9/10], Step [200/800], Loss: 0.0036\n",
      "Epoch [9/10], Step [210/800], Loss: 0.0034\n",
      "Epoch [9/10], Step [220/800], Loss: 0.0033\n",
      "Epoch [9/10], Step [230/800], Loss: 0.0034\n",
      "Epoch [9/10], Step [240/800], Loss: 0.0038\n",
      "Epoch [9/10], Step [250/800], Loss: 0.0033\n",
      "Epoch [9/10], Step [260/800], Loss: 0.0020\n",
      "Epoch [9/10], Step [270/800], Loss: 0.0017\n",
      "Epoch [9/10], Step [280/800], Loss: 0.0022\n",
      "Epoch [9/10], Step [290/800], Loss: 0.0021\n",
      "Epoch [9/10], Step [300/800], Loss: 0.0041\n",
      "Epoch [9/10], Step [310/800], Loss: 0.0031\n",
      "Epoch [9/10], Step [320/800], Loss: 0.0180\n",
      "Epoch [9/10], Step [330/800], Loss: 0.0015\n",
      "Epoch [9/10], Step [340/800], Loss: 0.0015\n",
      "Epoch [9/10], Step [350/800], Loss: 0.0017\n",
      "Epoch [9/10], Step [360/800], Loss: 0.0017\n",
      "Epoch [9/10], Step [370/800], Loss: 0.0014\n",
      "Epoch [9/10], Step [380/800], Loss: 0.0037\n",
      "Epoch [9/10], Step [390/800], Loss: 0.0024\n",
      "Epoch [9/10], Step [400/800], Loss: 0.0014\n",
      "Epoch [9/10], Step [410/800], Loss: 0.6367\n",
      "Epoch [9/10], Step [420/800], Loss: 0.2476\n",
      "Epoch [9/10], Step [430/800], Loss: 1.7902\n",
      "Epoch [9/10], Step [440/800], Loss: 0.1215\n",
      "Epoch [9/10], Step [450/800], Loss: 0.0059\n",
      "Epoch [9/10], Step [460/800], Loss: 0.0224\n",
      "Epoch [9/10], Step [470/800], Loss: 0.0133\n",
      "Epoch [9/10], Step [480/800], Loss: 0.0092\n",
      "Epoch [9/10], Step [490/800], Loss: 0.0092\n",
      "Epoch [9/10], Step [500/800], Loss: 0.0222\n",
      "Epoch [9/10], Step [510/800], Loss: 0.0014\n",
      "Epoch [9/10], Step [520/800], Loss: 0.0013\n",
      "Epoch [9/10], Step [530/800], Loss: 0.0018\n",
      "Epoch [9/10], Step [540/800], Loss: 0.0007\n",
      "Epoch [9/10], Step [550/800], Loss: 0.0009\n",
      "Epoch [9/10], Step [560/800], Loss: 0.0003\n",
      "Epoch [9/10], Step [570/800], Loss: 0.0010\n",
      "Epoch [9/10], Step [580/800], Loss: 0.0017\n",
      "Epoch [9/10], Step [590/800], Loss: 0.0010\n",
      "Epoch [9/10], Step [600/800], Loss: 0.0007\n",
      "Epoch [9/10], Step [610/800], Loss: 0.0009\n",
      "Epoch [9/10], Step [620/800], Loss: 0.0006\n",
      "Epoch [9/10], Step [630/800], Loss: 0.0008\n",
      "Epoch [9/10], Step [640/800], Loss: 0.0019\n",
      "Epoch [9/10], Step [650/800], Loss: 0.0004\n",
      "Epoch [9/10], Step [660/800], Loss: 0.0003\n",
      "Epoch [9/10], Step [670/800], Loss: 0.0003\n",
      "Epoch [9/10], Step [680/800], Loss: 0.0002\n",
      "Epoch [9/10], Step [690/800], Loss: 0.0005\n",
      "Epoch [9/10], Step [700/800], Loss: 0.0002\n",
      "Epoch [9/10], Step [710/800], Loss: 0.0005\n",
      "Epoch [9/10], Step [720/800], Loss: 0.0022\n",
      "Epoch [9/10], Step [730/800], Loss: 0.0001\n",
      "Epoch [9/10], Step [740/800], Loss: 0.0002\n",
      "Epoch [9/10], Step [750/800], Loss: 0.0001\n",
      "Epoch [9/10], Step [760/800], Loss: 0.0003\n",
      "Epoch [9/10], Step [770/800], Loss: 0.0010\n",
      "Epoch [9/10], Step [780/800], Loss: 0.0030\n",
      "Epoch [9/10], Step [790/800], Loss: 0.0013\n",
      "Epoch [9/10], Step [800/800], Loss: 0.0002\n",
      "Epoch [10/10], Step [10/800], Loss: 0.4340\n",
      "Epoch [10/10], Step [20/800], Loss: 0.0643\n",
      "Epoch [10/10], Step [30/800], Loss: 0.0427\n",
      "Epoch [10/10], Step [40/800], Loss: 0.5135\n",
      "Epoch [10/10], Step [50/800], Loss: 0.0211\n",
      "Epoch [10/10], Step [60/800], Loss: 0.0564\n",
      "Epoch [10/10], Step [70/800], Loss: 0.0041\n",
      "Epoch [10/10], Step [80/800], Loss: 0.0640\n",
      "Epoch [10/10], Step [90/800], Loss: 0.0084\n",
      "Epoch [10/10], Step [100/800], Loss: 0.1483\n",
      "Epoch [10/10], Step [110/800], Loss: 0.0043\n",
      "Epoch [10/10], Step [120/800], Loss: 0.0384\n",
      "Epoch [10/10], Step [130/800], Loss: 0.0060\n",
      "Epoch [10/10], Step [140/800], Loss: 0.0036\n",
      "Epoch [10/10], Step [150/800], Loss: 0.0818\n",
      "Epoch [10/10], Step [160/800], Loss: 0.0051\n",
      "Epoch [10/10], Step [170/800], Loss: 0.0018\n",
      "Epoch [10/10], Step [180/800], Loss: 0.0067\n",
      "Epoch [10/10], Step [190/800], Loss: 0.0035\n",
      "Epoch [10/10], Step [200/800], Loss: 0.0027\n",
      "Epoch [10/10], Step [210/800], Loss: 0.0025\n",
      "Epoch [10/10], Step [220/800], Loss: 0.0024\n",
      "Epoch [10/10], Step [230/800], Loss: 0.0028\n",
      "Epoch [10/10], Step [240/800], Loss: 0.0028\n",
      "Epoch [10/10], Step [250/800], Loss: 0.0026\n",
      "Epoch [10/10], Step [260/800], Loss: 0.0014\n",
      "Epoch [10/10], Step [270/800], Loss: 0.0010\n",
      "Epoch [10/10], Step [280/800], Loss: 0.0017\n",
      "Epoch [10/10], Step [290/800], Loss: 0.0014\n",
      "Epoch [10/10], Step [300/800], Loss: 0.0035\n",
      "Epoch [10/10], Step [310/800], Loss: 0.0026\n",
      "Epoch [10/10], Step [320/800], Loss: 0.0230\n",
      "Epoch [10/10], Step [330/800], Loss: 0.0010\n",
      "Epoch [10/10], Step [340/800], Loss: 0.0011\n",
      "Epoch [10/10], Step [350/800], Loss: 0.0013\n",
      "Epoch [10/10], Step [360/800], Loss: 0.0013\n",
      "Epoch [10/10], Step [370/800], Loss: 0.0009\n",
      "Epoch [10/10], Step [380/800], Loss: 0.0042\n",
      "Epoch [10/10], Step [390/800], Loss: 0.0017\n",
      "Epoch [10/10], Step [400/800], Loss: 0.0012\n",
      "Epoch [10/10], Step [410/800], Loss: 0.3195\n",
      "Epoch [10/10], Step [420/800], Loss: 0.2431\n",
      "Epoch [10/10], Step [430/800], Loss: 1.4666\n",
      "Epoch [10/10], Step [440/800], Loss: 0.0845\n",
      "Epoch [10/10], Step [450/800], Loss: 0.0028\n",
      "Epoch [10/10], Step [460/800], Loss: 0.0207\n",
      "Epoch [10/10], Step [470/800], Loss: 0.0114\n",
      "Epoch [10/10], Step [480/800], Loss: 0.0070\n",
      "Epoch [10/10], Step [490/800], Loss: 0.0086\n",
      "Epoch [10/10], Step [500/800], Loss: 0.0161\n",
      "Epoch [10/10], Step [510/800], Loss: 0.0009\n",
      "Epoch [10/10], Step [520/800], Loss: 0.0007\n",
      "Epoch [10/10], Step [530/800], Loss: 0.0014\n",
      "Epoch [10/10], Step [540/800], Loss: 0.0004\n",
      "Epoch [10/10], Step [550/800], Loss: 0.0006\n",
      "Epoch [10/10], Step [560/800], Loss: 0.0002\n",
      "Epoch [10/10], Step [570/800], Loss: 0.0005\n",
      "Epoch [10/10], Step [580/800], Loss: 0.0018\n",
      "Epoch [10/10], Step [590/800], Loss: 0.0006\n",
      "Epoch [10/10], Step [600/800], Loss: 0.0004\n",
      "Epoch [10/10], Step [610/800], Loss: 0.0005\n",
      "Epoch [10/10], Step [620/800], Loss: 0.0007\n",
      "Epoch [10/10], Step [630/800], Loss: 0.0005\n",
      "Epoch [10/10], Step [640/800], Loss: 0.0014\n",
      "Epoch [10/10], Step [650/800], Loss: 0.0003\n",
      "Epoch [10/10], Step [660/800], Loss: 0.0002\n",
      "Epoch [10/10], Step [670/800], Loss: 0.0002\n",
      "Epoch [10/10], Step [680/800], Loss: 0.0001\n",
      "Epoch [10/10], Step [690/800], Loss: 0.0003\n",
      "Epoch [10/10], Step [700/800], Loss: 0.0001\n",
      "Epoch [10/10], Step [710/800], Loss: 0.0002\n",
      "Epoch [10/10], Step [720/800], Loss: 0.0036\n",
      "Epoch [10/10], Step [730/800], Loss: 0.0001\n",
      "Epoch [10/10], Step [740/800], Loss: 0.0001\n",
      "Epoch [10/10], Step [750/800], Loss: 0.0000\n",
      "Epoch [10/10], Step [760/800], Loss: 0.0001\n",
      "Epoch [10/10], Step [770/800], Loss: 0.0012\n",
      "Epoch [10/10], Step [780/800], Loss: 0.0017\n",
      "Epoch [10/10], Step [790/800], Loss: 0.0008\n",
      "Epoch [10/10], Step [800/800], Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the ConvLSTM model\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.conv_lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h0, c0 = hidden\n",
    "        out, _ = self.conv_lstm(x, (h0, c0))\n",
    "        return out[:, -1]\n",
    "\n",
    "\n",
    "# Load pre-trained VGG19 model\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "vgg19_features = vgg19.features\n",
    "\n",
    "\n",
    "# Freeze the parameters in VGG19\n",
    "for param in vgg19_features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Combine VGG19 and ConvLSTM\n",
    "class ViolenceDetectionModel(nn.Module):\n",
    "    def __init__(self, vgg_features, hidden_dim, num_layers, num_classes):\n",
    "        super(ViolenceDetectionModel, self).__init__()\n",
    "        self.vgg_features = vgg_features\n",
    "        self.conv_lstm = ConvLSTM(512, hidden_dim, num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.vgg_features(x)\n",
    "        features = torch.reshape(features, (features.size(0), -1, features.size(1)))\n",
    "        h0 = torch.zeros(self.conv_lstm.num_layers, features.size(0), self.conv_lstm.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.conv_lstm.num_layers, features.size(0), self.conv_lstm.hidden_dim).to(x.device)\n",
    "        convlstm_out = self.conv_lstm(features, (h0, c0))  # Pass the hidden tuple\n",
    "        out = self.fc(convlstm_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Set hyperparameters\n",
    "input_dim = 512  # Dimensionality of input features from VGG19\n",
    "hidden_dim = 128  # Hidden dimension of ConvLSTM\n",
    "num_layers = 1  # Number of ConvLSTM layers\n",
    "num_classes = 2  # Number of output classes (violence, non-violence)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "\n",
    "# Instantiate the model\n",
    "model = ViolenceDetectionModel(vgg19_features, hidden_dim, num_layers, num_classes)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Dataset directory containing video clips\n",
    "dataset_dir = 'C:/APIIT BENG SE Degree/Year 3/Final Year Project/Hockey'\n",
    "\n",
    "# List video files in the dataset directory\n",
    "violent_files = [f for f in os.listdir(os.path.join(dataset_dir, 'violent')) if f.endswith('.avi')]\n",
    "non_violent_files = [f for f in os.listdir(os.path.join(dataset_dir, 'non-violent')) if f.endswith('.avi')]\n",
    "\n",
    "# Combine the lists\n",
    "video_files = violent_files + non_violent_files\n",
    "\n",
    "# Create labels for the videos\n",
    "labels = [1] * len(violent_files) + [0] * len(non_violent_files)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, video_file in enumerate(video_files):\n",
    "        # Determine the label for the video\n",
    "        label = labels[i]\n",
    "\n",
    "        # Open video file\n",
    "        if label == 1:\n",
    "            video_path = os.path.join(dataset_dir, 'violent', video_file)\n",
    "        else:\n",
    "            video_path = os.path.join(dataset_dir, 'non-violent', video_file)\n",
    "\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Preprocess frame (resize, normalize, etc.)\n",
    "            frame = cv2.resize(frame, (224, 224))  # Adjust size as per VGG19 input requirements\n",
    "            frame = frame.astype(np.float32) / 255.0  # Normalize pixel values between 0 and 1\n",
    "            frame = torch.from_numpy(frame).permute(2, 0, 1).unsqueeze(0).to(device)  # Convert to tensor\n",
    "\n",
    "            frames.append(frame)\n",
    "\n",
    "        video.release()\n",
    "\n",
    "        # Stack frames to create a tensor with shape (num_frames, channels, height, width)\n",
    "        frames = torch.cat(frames, dim=0)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(frames)\n",
    "\n",
    "        # Generate target labels\n",
    "        target = torch.tensor([label] * outputs.size(0), dtype=torch.long).to(device)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training progress\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(video_files)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'violence_detection_model_v2.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85dad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
